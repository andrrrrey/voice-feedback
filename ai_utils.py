import json
import logging
import os
from typing import Tuple

import requests
from dotenv import load_dotenv

load_dotenv()

YANDEX_API_KEY = os.getenv("YANDEX_API_KEY")
YANDEX_FOLDER_ID = os.getenv("YANDEX_FOLDER_ID")
YANDEX_SPEECHKIT_STT_URL = os.getenv(
    "YANDEX_SPEECHKIT_STT_URL",
    "https://stt.api.cloud.yandex.net/speech/v1/stt:recognize",
)
YANDEX_SPEECHKIT_LANG = os.getenv("YANDEX_SPEECHKIT_LANG", "ru-RU")

# üîπ –î–û–ë–ê–í–ò–õ: —Ñ–æ—Ä–º–∞—Ç –∞—É–¥–∏–æ –¥–ª—è STT (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ‚Äî oggopus)
YANDEX_SPEECHKIT_FORMAT = os.getenv("YANDEX_SPEECHKIT_FORMAT", "oggopus")

YANDEX_GPT_URL = os.getenv(
    "YANDEX_GPT_URL",
    "https://llm.api.cloud.yandex.net/foundationModels/v1/completion",
)
YANDEX_GPT_MODEL = os.getenv("YANDEX_GPT_MODEL", "yandexgpt-lite")

# –ü—Ä–æ–º–ø—Ç—ã –º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
NORMALIZATION_PROMPT = os.getenv(
    "YANDEX_GPT_NORMALIZATION_PROMPT",
    "–ù–æ—Ä–º–∞–ª–∏–∑—É–π —Ç–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞: —É–±–µ—Ä–∏ –º–µ–∂–¥–æ–º–µ—Ç–∏—è, –∏—Å–ø—Ä–∞–≤—å –æ–ø–µ—á–∞—Ç–∫–∏ –∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é,"
    " —Å–æ—Ö—Ä–∞–Ω–∏ —Å–º—ã—Å–ª. –í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç.",
)
SENTIMENT_PROMPT = os.getenv(
    "YANDEX_GPT_SENTIMENT_PROMPT",
    "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –æ—Ç–∑—ã–≤–æ–≤. –û–ø—Ä–µ–¥–µ–ª–∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞ –∫–∞–∫"
    " positive, neutral –∏–ª–∏ negative, –æ–ø–∏—Ä–∞—è—Å—å –Ω–∞ –æ–±—â–∏–π —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –æ–∫—Ä–∞—Å –∏"
    " –æ—Ü–µ–Ω–æ—á–Ω—ã–µ —Å—É–∂–¥–µ–Ω–∏—è. –£—á–∏—Ç—ã–≤–∞–π –≤–µ—Å—å –∫–æ–Ω—Ç–µ–∫—Å—Ç, –ø—Ä–æ—Ç–∏–≤–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è (\"–Ω–æ\"," 
    "\"–æ–¥–Ω–∞–∫–æ\", \"–∑–∞—Ç–æ\"), –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º—ã—Å–ª–µ–π –∏ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –≤—ã–≤–æ–¥"
    " –∞–≤—Ç–æ—Ä–∞."
    "\n–ü—Ä–∞–≤–∏–ª–∞:\n"
    "- negative: –∂–∞–ª–æ–±—ã, –Ω–µ—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—ë–Ω–Ω–æ—Å—Ç—å, –º–µ–¥–ª–µ–Ω–Ω–æ–µ/–ø–ª–æ—Ö–æ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ,"
    " —É–≥—Ä–æ–∑—ã –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å—Å—è –∏–ª–∏ –Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞—Ç—å, –¥–∞–∂–µ –µ—Å–ª–∏ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω—ã–µ"
    " –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞.\n"
    "- positive: —è–≤–Ω–∞—è –ø–æ—Ö–≤–∞–ª–∞, –≤—ã—Å–æ–∫–∞—è –æ—Ü–µ–Ω–∫–∞, –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞—Ç—å.\n"
    "- neutral: –æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω–∞—è —Ä–µ—á—å –±–µ–∑ —è–≤–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫ –∏–ª–∏ –∫–æ–≥–¥–∞ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ –∏"
    " –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –≤ —Ä–∞–≤–Ω–æ–≤–µ—Å–∏–∏ –∏ –∞–≤—Ç–æ—Ä –Ω–µ –≤—ã—Ä–∞–∂–∞–µ—Ç –∏—Ç–æ–≥–æ–≤–æ–µ –Ω–µ–¥–æ–≤–æ–ª—å—Å—Ç–≤–æ"
    " –∏–ª–∏ –≤–æ—Å—Ç–æ—Ä–≥."
    "\n–ü—Ä–∏–º–µ—Ä—ã:\n"
    "\"–£–∂–∞—Å–Ω–∞—è –∫–æ–º–ø–∞–Ω–∏—è, —Å–µ—Ä–≤–∏—Å –º–µ–¥–ª–µ–Ω–Ω—ã–π, –±–æ–ª—å—à–µ –Ω–µ –ø—Ä–∏–¥—É\" -> negative\n"
    "\"–°–µ—Ä–≤–∏—Å —Ö–æ—Ä–æ—à–∏–π, –µ–¥–∞ –≤–∫—É—Å–Ω–∞—è, —Ä–µ–∫–æ–º–µ–Ω–¥—É—é\" -> positive\n"
    "\"–û–±—ã—á–Ω–æ–µ –∫–∞—Ñ–µ, —Å—Ä–µ–¥–Ω–∏–µ —Ü–µ–Ω—ã, –Ω–∏—á–µ–≥–æ –æ—Å–æ–±–µ–Ω–Ω–æ–≥–æ\" -> neutral\n"
    "\"–ë—ã–ª–∏ –∑–∞–¥–µ—Ä–∂–∫–∏, –Ω–æ –≤ –∏—Ç–æ–≥–µ –≤—Å—ë —Å–¥–µ–ª–∞–ª–∏ –∏ —è –¥–æ–≤–æ–ª–µ–Ω\" -> positive\n"
    "\"–ï—Å—Ç—å –ø–∞—Ä—É –ø–ª—é—Å–æ–≤, –æ–¥–Ω–∞–∫–æ –≤ —Ü–µ–ª–æ–º –æ—Å—Ç–∞–ª—Å—è –Ω–µ–¥–æ–≤–æ–ª–µ–Ω –∏ –Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é\""
    " -> negative\n"
    "–í—Å–µ–≥–¥–∞ –≤—ã–±–∏—Ä–∞–π –º–µ—Ç–∫—É, –æ—Ç—Ä–∞–∂–∞—é—â—É—é –æ–±—â–∏–π –≤—ã–≤–æ–¥ –∞–≤—Ç–æ—Ä–∞, –æ—Å–æ–±–µ–Ω–Ω–æ —Ñ–∏–Ω–∞–ª—å–Ω—É—é"
    " –æ—Ü–µ–Ω–∫—É.",
)

logger = logging.getLogger(__name__)


def _auth_headers() -> dict:
    if not YANDEX_API_KEY:
        raise RuntimeError("YANDEX_API_KEY –Ω–µ –∑–∞–¥–∞–Ω")
    return {"Authorization": f"Api-Key {YANDEX_API_KEY}"}


def transcribe_audio_with_speechkit(audio_path: str) -> str:
    """
    –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∞—É–¥–∏–æ—Ñ–∞–π–ª –≤ Yandex SpeechKit –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç.
    –û–∂–∏–¥–∞–µ—Ç—Å—è, —á—Ç–æ –Ω–∞ –≤—Ö–æ–¥ –ø–æ–¥–∞—ë—Ç—Å—è OGG Opus (format=oggopus).
    """

    if not os.path.exists(audio_path):
        raise FileNotFoundError(f"Audio file not found: {audio_path}")

    headers = _auth_headers()
    headers["Content-Type"] = "application/octet-stream"
    if not YANDEX_FOLDER_ID:
        raise RuntimeError("YANDEX_FOLDER_ID –Ω–µ –∑–∞–¥–∞–Ω")

    # üîπ –î–û–ë–ê–í–ò–õ: —è–≤–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç, —á—Ç–æ–±—ã SpeechKit –∂–¥–∞–ª oggopus,
    # –∞ –Ω–µ –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π ogg –±–µ–∑ —É—Ç–æ—á–Ω–µ–Ω–∏—è.
    params = {
        "folderId": YANDEX_FOLDER_ID,
        "lang": YANDEX_SPEECHKIT_LANG,
        "format": YANDEX_SPEECHKIT_FORMAT,
    }

    with open(audio_path, "rb") as f:
        audio_bytes = f.read()

    response = requests.post(
        YANDEX_SPEECHKIT_STT_URL,
        params=params,
        headers=headers,
        data=audio_bytes,
        timeout=30,
    )

    if response.status_code != 200:
        # üîπ –ß—É—Ç—å –±–æ–ª–µ–µ –≥–æ–≤–æ—Ä—è—â–∏–π –ª–æ–≥: —Å—Ç–∞—Ç—É—Å + —Ç–µ–ª–æ
        logger.error(
            "SpeechKit error (status=%s): %s",
            response.status_code,
            response.text,
        )
        raise RuntimeError(
            f"SpeechKit STT request failed with status {response.status_code}"
        )

    payload = response.json()
    if payload.get("error_code"):
        raise RuntimeError(
            f"SpeechKit STT error {payload.get('error_code')}:"
            f" {payload.get('error_message')}"
        )

    return (payload.get("result") or "").strip()


def _build_gpt_prompt() -> str:
    return (
        f"{NORMALIZATION_PROMPT}\n\n"
        f"{SENTIMENT_PROMPT}\n"
        "–û—Ç–≤–µ—Ç—å —Å—Ç—Ä–æ–≥–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON: "
        '{"normalized_text": "...", "sentiment": "positive|neutral|negative"}.\n'
        "–ù–µ –¥–æ–±–∞–≤–ª—è–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏ –ø–æ—è—Å–Ω–µ–Ω–∏—è."
    )


def _heuristic_sentiment_from_text(text: str) -> str:
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –¢–û–õ–¨–ö–û –ø–æ —Ç–µ–∫—Å—Ç—É, –±–µ–∑ —É—á—ë—Ç–∞ –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: 'positive', 'neutral' –∏–ª–∏ 'negative'.
    """
    t = (text or "").lower()

    negative_markers = [
        "–Ω–µ –ø–æ–Ω—Ä–∞–≤",          # "–Ω–µ –ø–æ–Ω—Ä–∞–≤–∏–ª–∏—Å—å —É—Å–ª—É–≥–∏", "–Ω–µ –ø–æ–Ω—Ä–∞–≤–∏–ª–æ—Å—å"
        "—É–∂–∞—Å–Ω",              # "—É–∂–∞—Å–Ω—ã–π", "—É–∂–∞—Å–Ω–æ"
        "–æ—Ç–≤—Ä–∞—Ç",             # "–æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω–æ"
        "–∫–æ—à–º–∞—Ä",
        "–ø–ª–æ—Ö–æ–π", "–ø–ª–æ—Ö–∞—è", "–ø–ª–æ—Ö–∏–µ", "–ø–ª–æ—Ö–æ",
        "–º–µ–¥–ª–µ–Ω–Ω",            # "–º–µ–¥–ª–µ–Ω–Ω–æ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ"
        "—Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω", "—Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω–∞", "—Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω–∏–µ",
        "–Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥",        # "–Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é", "–Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–ª"
        "–Ω–µ —Å–æ–≤–µ—Ç—É—é",
        "–±–æ–ª—å—à–µ –Ω–µ –ø—Ä–∏–¥—É",
        "–Ω–∏–∫–æ–≥–¥–∞ –±–æ–ª—å—à–µ",
        "–æ–±–º–∞–Ω", "—Ä–∞–∑–≤–æ–¥",
        "—Ö–∞–º—Å—Ç–≤",
        "–Ω–∞–ø–ª–µ–≤–∞—Ç–µ–ª—å—Å–∫",      # "–Ω–∞–ø–ª–µ–≤–∞—Ç–µ–ª—å—Å–∫–∏"
    ]

    positive_markers = [
        # —É–±—Ä–∞–ª–∏ –≥–æ–ª–æ–µ "–ø–æ–Ω—Ä–∞–≤", —á—Ç–æ–±—ã –Ω–µ –ª–æ–≤–∏—Ç—å "–Ω–µ –ø–æ–Ω—Ä–∞–≤–∏–ª–æ—Å—å"
        "–æ—á–µ–Ω—å –ø–æ–Ω—Ä–∞–≤–∏–ª–æ—Å—å",
        "–º–Ω–µ –ø–æ–Ω—Ä–∞–≤–∏–ª–æ—Å—å",
        "–Ω–∞–º –ø–æ–Ω—Ä–∞–≤–∏–ª–æ—Å—å",
        "–ø–æ–Ω—Ä–∞–≤–∏–ª–æ—Å—å –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ",
        "–æ—Ç–ª–∏—á–Ω",
        "–ø—Ä–µ–∫—Ä–∞—Å–Ω",
        "–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω",
        "—Å—É–ø–µ—Ä",
        "–∫–ª–∞—Å—Å–Ω",
        "—à–∏–∫–∞—Ä–Ω",
        "–¥–æ–≤–æ–ª–µ–Ω", "–¥–æ–≤–æ–ª—å–Ω–∞",
        "–æ—á–µ–Ω—å –¥–æ–≤–æ–ª–µ–Ω", "–æ—á–µ–Ω—å –¥–æ–≤–æ–ª—å–Ω–∞",
        "—Ä–µ–∫–æ–º–µ–Ω–¥—É—é",
        "—Å–æ–≤–µ—Ç—É—é",
        "–±—É–¥—É –æ–±—Ä–∞—â–∞—Ç—å—Å—è –µ—â–µ", "–±—É–¥—É –æ–±—Ä–∞—â–∞—Ç—å—Å—è –µ—â—ë",
        "–±—É–¥—É –ø—Ä–∏—Ö–æ–¥–∏—Ç—å –µ—â–µ", "–±—É–¥—É –ø—Ä–∏—Ö–æ–¥–∏—Ç—å –µ—â—ë",
        "–ª—É—á—à–∞—è", "–ª—É—á—à–∏–π —Å–µ—Ä–≤–∏—Å",
        "—Å–ø–∞—Å–∏–±–æ", "–±–ª–∞–≥–æ–¥–∞—Ä",
        "–≤—Å—ë –ø–æ–Ω—Ä–∞–≤–∏–ª–æ—Å—å", "–≤—Å–µ –ø–æ–Ω—Ä–∞–≤–∏–ª–æ—Å—å",
    ]

    has_neg = any(marker in t for marker in negative_markers)

    # –µ—Å–ª–∏ —è–≤–Ω—ã–π –Ω–µ–≥–∞—Ç–∏–≤ –µ—Å—Ç—å ‚Äî –æ–Ω –≤–∞–∂–Ω–µ–µ –≤—Å–µ–≥–æ
    if has_neg:
        return "negative"

    has_pos = any(marker in t for marker in positive_markers)

    if has_pos:
        return "positive"

    return "neutral"


def _parse_gpt_response(text: str, fallback: str) -> Tuple[str, str]:
    try:
        data = json.loads(text)
        normalized = (data.get("normalized_text") or fallback).strip()
        sentiment = (data.get("sentiment") or "neutral").strip()
        return normalized, sentiment
    except json.JSONDecodeError:
        logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–æ–±—Ä–∞—Ç—å –æ—Ç–≤–µ—Ç YandexGPT, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∑–∞–≥–ª—É—à–∫—É")
        return fallback, "neutral"


def normalize_and_analyze_with_yandex_gpt(raw_text: str) -> Tuple[str, str]:
    """
    –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Ç–µ–∫—Å—Ç –≤ YandexGPT –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (normalized_text, sentiment).
    """
    raw_text = raw_text or ""

    if not YANDEX_FOLDER_ID:
        raise RuntimeError("YANDEX_FOLDER_ID –Ω–µ –∑–∞–¥–∞–Ω")

    headers = {"Content-Type": "application/json"}
    headers.update(_auth_headers())

    model_uri = f"gpt://{YANDEX_FOLDER_ID}/{YANDEX_GPT_MODEL}"
    body = {
        "modelUri": model_uri,
        "completionOptions": {
            "stream": False,
            "temperature": 0.2,
            "maxTokens": 800,
        },
        "messages": [
            {
                "role": "system",
                "text": _build_gpt_prompt(),
            },
            {"role": "user", "text": raw_text},
        ],
    }

    response = requests.post(YANDEX_GPT_URL, headers=headers, json=body, timeout=30)
    if response.status_code != 200:
        logger.error("YandexGPT error: %s", response.text)
        return raw_text.strip(), "neutral"

    payload = response.json()
    try:
        text = payload["result"]["alternatives"][0]["message"]["text"]
    except (KeyError, IndexError):
        logger.warning("–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ YandexGPT: %s", payload)
        normalized = raw_text.strip()
        sentiment = _heuristic_sentiment_from_text(normalized)
        return normalized, sentiment

    normalized, _ = _parse_gpt_response(text, raw_text.strip())
    sentiment = _heuristic_sentiment_from_text(normalized)
    return normalized, sentiment
    